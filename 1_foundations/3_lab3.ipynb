{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "varunmarora@gmail.com\n",
      "www.linkedin.com/in/va55\n",
      "(LinkedIn)\n",
      "Top Skills\n",
      "Software Design\n",
      "Web Engineering\n",
      "Software Product Management\n",
      "Certifications\n",
      "Certified SAFe® Product Owner /\n",
      "Product Manager\n",
      "Varun Arora\n",
      "Product Manager\n",
      "San Francisco Bay Area\n",
      "Summary\n",
      "Finance grad turned software engineer turned product manager.\n",
      "Working with big clients to make their websites simple and easy to\n",
      "use!\n",
      "Experience\n",
      "Appvise Consulting\n",
      "10 years 2 months\n",
      "Product Manager, AEM\n",
      "November 2022 - Present (3 years 1 month)\n",
      "San Francisco Bay Area\n",
      "Product Manager overseeing cross-functional teams to scale enterprise SaaS\n",
      "platforms, combining\n",
      "data-driven prioritization, experimentation, and accessibility to deliver\n",
      "measurable improvements\n",
      "- Directed cross-functional development teams to enhance Adobe Experience\n",
      "Manager (AEM),\n",
      "delivering major features that impact 400k+ daily users across multiple\n",
      "business units\n",
      "- Developed and implemented a prioritization framework to evaluate\n",
      "enhancement requests,\n",
      "reducing backlog grooming time by 40%, and accelerating delivery of high-\n",
      "impact features by\n",
      "25%\n",
      "- Redesigned end-to-end onboarding experience in AEM, cutting time-to-value\n",
      "by 30% and\n",
      "boosting customer retention; complemented with scalable documentation that\n",
      "reduced\n",
      "onboarding time by 40%\n",
      "- Reduced system errors by 35% and stabilized codebase for the platforms two\n",
      "most-used widgets\n",
      "and improving overall site reliability\n",
      "  Page 1 of 2   \n",
      "- Achieved full WCAG 2.1 and ADA compliance, boosting accessibility usability\n",
      "by 25% and\n",
      "mitigating risk of future audit violations\n",
      "- Drove a 10% increase in page views by implementing data-driven functional\n",
      "design\n",
      "enhancements that improved user experience and engagement\n",
      "- Improved search accuracy by 20% through SEO and backend optimizations,\n",
      "driving higher\n",
      "content discoverability and engagement\n",
      "- Developed experimentation framework using Adobe Target to run 15+\n",
      "concurrent A/B tests;\n",
      "optimized key user flows and increased conversion rates by 18%.\n",
      "- Leveraged behavioral analytics (Medallia, Adobe Analytics) to prioritize\n",
      "roadmap decisions,\n",
      "identify friction points, and monitor product KPIs\n",
      "Software Engineer, Front-End\n",
      "October 2015 - October 2022 (7 years 1 month)\n",
      "San Francisco Bay Area\n",
      "Designed and delivered scalable SaaS applications for clients\n",
      "- Delivered mission-critical emergency status platform with 55M+ views and\n",
      "99.9% uptime,\n",
      "providing reliable customer communication during outages\n",
      "- Modernized customer-facing applications, driving a 35% increase in form\n",
      "completion rates and\n",
      "improving CSAT scores by 25%\n",
      "- Created Python scripts to automate the process of integrating PDFs with a\n",
      "clients website, saving ~2 hours per batch upload\n",
      "Education\n",
      "Menlo College\n",
      "Bachelor of Science - BS, Finance, General · (August 2013 - May 2015)\n",
      "  Page 2 of 2\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f: #first input calls the file, second input reads the file, third input is the encoding. The with command is used to open the file and close it automatically after the block is executed.\n",
    "    summary = f.read() #creating a variable called summary and reading the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Varun Arora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Varun Arora. You are answering questions on Varun Arora's website, particularly questions related to Varun Arora's career, background, skills and experience. Your responsibility is to represent Varun Arora for interactions on the website as faithfully as possible. You are given a summary of Varun Arora's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Varun Arora. I was born in Mumbai, India and moved to America with my family at the age of 4. I'm defined, not by my job title, but by my grit, desire to learn, and a deeprooted wish to see reduction in suffering around the world.\\nLets talk about something more light-hearted. I LOVE chai and coffee. I'm particularly fond of my girlfriends masala chai, and South Indian filter cofffee. My go-to place for filter coffee is Mylapore.\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\nvarunmarora@gmail.com\\nwww.linkedin.com/in/va55\\n(LinkedIn)\\nTop Skills\\nSoftware Design\\nWeb Engineering\\nSoftware Product Management\\nCertifications\\nCertified SAFe® Product Owner /\\nProduct Manager\\nVarun Arora\\nProduct Manager\\nSan Francisco Bay Area\\nSummary\\nFinance grad turned software engineer turned product manager.\\nWorking with big clients to make their websites simple and easy to\\nuse!\\nExperience\\nAppvise Consulting\\n10 years 2 months\\nProduct Manager, AEM\\nNovember 2022\\xa0-\\xa0Present\\xa0(3 years 1 month)\\nSan Francisco Bay Area\\nProduct Manager overseeing cross-functional teams to scale enterprise SaaS\\nplatforms, combining\\ndata-driven prioritization, experimentation, and accessibility to deliver\\nmeasurable improvements\\n- Directed cross-functional development teams to enhance Adobe Experience\\nManager (AEM),\\ndelivering major features that impact 400k+ daily users across multiple\\nbusiness units\\n- Developed and implemented a prioritization framework to evaluate\\nenhancement requests,\\nreducing backlog grooming time by 40%, and accelerating delivery of high-\\nimpact features by\\n25%\\n- Redesigned end-to-end onboarding experience in AEM, cutting time-to-value\\nby 30% and\\nboosting customer retention; complemented with scalable documentation that\\nreduced\\nonboarding time by 40%\\n- Reduced system errors by 35% and stabilized codebase for the platforms two\\nmost-used widgets\\nand improving overall site reliability\\n\\xa0 Page 1 of 2\\xa0 \\xa0\\n- Achieved full WCAG 2.1 and ADA compliance, boosting accessibility usability\\nby 25% and\\nmitigating risk of future audit violations\\n- Drove a 10% increase in page views by implementing data-driven functional\\ndesign\\nenhancements that improved user experience and engagement\\n- Improved search accuracy by 20% through SEO and backend optimizations,\\ndriving higher\\ncontent discoverability and engagement\\n- Developed experimentation framework using Adobe Target to run 15+\\nconcurrent A/B tests;\\noptimized key user flows and increased conversion rates by 18%.\\n- Leveraged behavioral analytics (Medallia, Adobe Analytics) to prioritize\\nroadmap decisions,\\nidentify friction points, and monitor product KPIs\\nSoftware Engineer, Front-End\\nOctober 2015\\xa0-\\xa0October 2022\\xa0(7 years 1 month)\\nSan Francisco Bay Area\\nDesigned and delivered scalable SaaS applications for clients\\n- Delivered mission-critical emergency status platform with 55M+ views and\\n99.9% uptime,\\nproviding reliable customer communication during outages\\n- Modernized customer-facing applications, driving a 35% increase in form\\ncompletion rates and\\nimproving CSAT scores by 25%\\n- Created Python scripts to automate the process of integrating PDFs with a\\nclients website, saving ~2 hours per batch upload\\nEducation\\nMenlo College\\nBachelor of Science - BS,\\xa0Finance, General\\xa0·\\xa0(August 2013\\xa0-\\xa0May 2015)\\n\\xa0 Page 2 of 2\\n\\nWith this context, please chat with the user, always staying in character as Varun Arora.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history): #Wrapper for the OpenAI API.\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.0-flash\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
